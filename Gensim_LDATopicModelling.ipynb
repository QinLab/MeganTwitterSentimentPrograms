{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Breaks the large raw tweet data file into manageable portions\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#setting file size to 10000 tweets\n",
    "chunk_size = 10000\n",
    "batch_no = 1\n",
    "\n",
    "print(\"Beginning to parse tweets...\")\n",
    "\n",
    "#Adjust path name per file, this is the small test file I am running currently\n",
    "for chunk in pd.read_csv(\"./tweets_01-04.csv\", chunksize = chunk_size, error_bad_lines=False):\n",
    "        chunk.to_csv(\"covid_data\" + str(batch_no) + \".csv\", index = False)\n",
    "        batch_no += 1\n",
    "\n",
    "print(\"Finished parsing tweets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import csv\n",
    "import glob\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import spacy\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " creates corpus for topic modelling \n",
    "'''\n",
    "import re\n",
    "\n",
    "#Correct Dates start at file 700\n",
    "\n",
    "sentimentDict = {}\n",
    "documents = []\n",
    "path = \"./covid_data*\"\n",
    "\n",
    "num_files = 0\n",
    "\n",
    "print(\"Beginning to create corpus...\")\n",
    "#I put the smaller chunked tweet files in to a directory called revisedCovidData\n",
    "for filename in tqdm(glob.glob(path)):\n",
    "    if int(filename[12:-4]) >= 700:\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as rawTweets:\n",
    "            #open as CSV iterator\n",
    "            readCSV = csv.reader(rawTweets)\n",
    "            next(readCSV)\n",
    "            #Iterate through individual tweets\n",
    "            tweet_count = 0\n",
    "            for line in readCSV:\n",
    "                if tweet_count%100 == 0:\n",
    "                    result = re.sub(r\"http\\S+\", \"\", line[1])\n",
    "                    documents.append(result)\n",
    "                tweet_count += 1\n",
    "    #break    \n",
    "        \n",
    "print(len(documents))\n",
    "print(documents[-21])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and token not in [\"rtrt\", \"https\", 'rt']:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sample = documents[-21]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessed_docs = []\n",
    "print(\"processing the documents\")\n",
    "for tweet in tqdm(documents):\n",
    "    preprocessed_docs.append(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Creating dictionary\")\n",
    "dictionary = gensim.corpora.Dictionary(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filters out any token in 15 or fewer, more than half, and only the 15,000 most common\n",
    "dictionary.filter_extremes(no_below = 15, no_above = 0.5, keep_n = 15000)\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessed_docs[5])\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "id2word = corpora.Dictionary(preprocessed_docs)\n",
    "texts = preprocessed_docs\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import os\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "os.environ.update({'MALLET_HOME':r'C:/Users/user/Documents/TopicModelling/Covid_Data/mallet-2.0.8'}) \n",
    "#You should update this path as per the path of Mallet directory on your system.\n",
    "mallet_path = r'C:/Users/user/Documents/TopicModelling/Covid_Data/mallet-2.0.8/bin/mallet' \n",
    "#You should update this path as per the path of Mallet directory on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamallet = gensim.models.wrappers.LdaMallet(\n",
    "   mallet_path, corpus=corpus, num_topics=20, id2word=id2word\n",
    ")\n",
    "pprint(ldamallet.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11 = reopen, 5 = stay home for ldamallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_topic(text, topic_num):\n",
    "    tokens = preprocess(text.lower())\n",
    "    return ldamallet[dictionary.doc2bow(token for token in tokens)][topic_num][1]\n",
    "def update(date):\n",
    "    u = dt.datetime.strptime(\"2011-01-01\",\"%Y-%m-%d\")\n",
    "    d = dt.timedelta(days=7)\n",
    "    t = u + d\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import textblob\n",
    "import glob\n",
    "\n",
    "sentimentDict = {}\n",
    "path = \"./covid_data*\"\n",
    "abbr_list = ['ak', 'al', 'ar', 'az', 'ca', 'co', \n",
    "             'ct', 'dc', 'de', 'fl', 'ga', 'hi', \n",
    "             'ia', 'id', 'il', 'in', 'ks', 'ky', \n",
    "             'la', 'ma', 'md', 'me', 'mi', 'mn', \n",
    "             'ms', 'mo', 'mt', 'nc', 'ne', 'nh', \n",
    "             'nj', 'nm', 'nv', 'ny', 'nd', 'oh', \n",
    "             'ok', 'or', 'pa', 'ri', 'sc', 'sd', \n",
    "             'tn', 'tx', 'ut', 'vt', 'va', 'wa', \n",
    "             'wv', 'wi', 'wy']\n",
    "\n",
    "column_names = [\"Start Date\", \"State\", \"Social Dist Sentiment\", \"Reopening Sentiment\", \"Other Sentiments\"]\n",
    "\n",
    "#starting one week before start date so it updates correctly\n",
    "first = dt.datetime(2020, 1, 20)\n",
    "weekStart = first\n",
    "weekEnd = update(weekStart)\n",
    "weekString = ''\n",
    "startDate = dt.datetime(2020, 1, 27)\n",
    "\n",
    "\n",
    "for filename in tqdm(glob.glob(path)):\n",
    "    #start at document 700, as any earlier is outside our range, thus futile\n",
    "    if int(filename[12:-4]) >= 700:\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as rawTweets:\n",
    "            #open as CSV iterator\n",
    "            readCSV = csv.reader(rawTweets)\n",
    "            for line in readCSV:\n",
    "                next(readCSV)\n",
    "                #change date into datetime object\n",
    "                #Format == Thu Jan 23 15:41:43 +0000 2020 \n",
    "                date = dt.datetime.strptime(line[4], '%a %b %d %H:%M:%S %z %Y')\n",
    "                #make sure they are the same week\n",
    "                if date >= weekStart and date < weekEnd:\n",
    "                    weekString = dt.strftime(weekStart)\n",
    "                    if line[9] != \"Null\" or \"us_state\":\n",
    "                        if is_topic(line[1], 5) > 0.05:\n",
    "                            #calls text of each tweet as a TextBlob object\n",
    "                            if sentimementDict[weekString][line[9]][0] != 0:\n",
    "                                avgSentiment = (sentimementDict[weekString][line[9]][0] +  (text.sentiment.polarity))/2\n",
    "                            else:\n",
    "                                avgSentiment = (text.sentiment.polarity)\n",
    "                            sentimementDict[weekString][line[9]][0] = avgSentiment\n",
    "                        elif is_topic(line[1], 11) > 0.05:\n",
    "                            #calls text of each tweet as a TextBlob object\n",
    "                            text = textblob.TextBlob(line[1])\n",
    "                            #line[9] = state; if this state is already in the dictionary, the sentiment gets averaged\n",
    "                            if sentimementDict[weekString][line[9]][1] != 0:\n",
    "                                avgSentiment = (sentimementDict[weekString][line[9]][1] +  (text.sentiment.polarity))/2\n",
    "                            else:\n",
    "                                avgSentiment = (text.sentiment.polarity)\n",
    "                            sentimementDict[weekString][line[9]][1] = avgSentiment\n",
    "                        else:\n",
    "                            #calls text of each tweet as a TextBlob object\n",
    "                            text = textblob.TextBlob(line[1])\n",
    "                            #line[9] = state; if this state is already in the dictionary, the sentiment gets averaged\n",
    "                            if sentimementDict[weekString][line[9]][2] != 0:\n",
    "                                avgSentiment = (sentimementDict[weekString][line[9]][2] +  (text.sentiment.polarity))/2\n",
    "                            else:\n",
    "                                avgSentiment = (text.sentiment.polarity)\n",
    "                            sentimementDict[weekString][line[9]][2] = avgSentiment\n",
    "                #if tweet is from the next week, updates the start date\n",
    "                elif date >= weekEnd:\n",
    "                    #add a week\n",
    "                    weekStart = update(weekStart)\n",
    "                    #create a dictionary with each state as key and a list of 0's as values\n",
    "                    statesDict = {}\n",
    "                    #iterate each state name into the keys\n",
    "                    for state in abbr_list:\n",
    "                        #initialize the dictionary\n",
    "                        statesDict[state] = [0, 0, 0]\n",
    "                    #add this dictionary to the larger one\n",
    "                    weekString = dt.strftime(weekStart)\n",
    "                    sentimentDict.update({weekString, statesDict})\n",
    "                    #make sure this is a line with data\n",
    "                    if line[9] != \"Null\" or \"us_state\":\n",
    "                        if is_topic(line[1], 5) > 0.05:\n",
    "                            #calls text of each tweet as a TextBlob object\n",
    "                            avgSentiment = (text.sentiment.polarity)\n",
    "                            sentimementDict[weekString][line[9]][0] = avgSentiment\n",
    "                        elif is_topic(line[1], 11) > 0.05:\n",
    "                            #calls text of each tweet as a TextBlob object\n",
    "                            text = textblob.TextBlob(line[1])\n",
    "                            #line[9] = state; if this state is already in the dictionary, the sentiment gets averaged\n",
    "                            avgSentiment = (text.sentiment.polarity)\n",
    "                            sentimementDict[weekString][line[9]][1] = avgSentiment\n",
    "                        else:\n",
    "                            #calls text of each tweet as a TextBlob object\n",
    "                            text = textblob.TextBlob(line[1])\n",
    "                            #line[9] = state; if this state is already in the dictionary, the sentiment gets averaged\n",
    "                            avgSentiment = (text.sentiment.polarity)\n",
    "                            sentimementDict[weekString][line[9]][2] = avgSentiment\n",
    "                elif date <= weekStart and date >= startDate:\n",
    "                    oneWeek = dt.timedelta(days = 7)\n",
    "                    previousWeekEnd = weekEnd - oneWeek\n",
    "                    while (date >= previousWeekEnd):\n",
    "                        previousWeekEnd = previousWeekEnd - oneWeek\n",
    "                        previousWeekStart = previousWeekEnd - oneWeek\n",
    "                    earlierWeekString = dt.strftime(previousWeekStart)\n",
    "                    if earlierWeekString not in SenimentDict:\n",
    "                        sentimentDict.update({earlierWeekString, statesDict})\n",
    "                        #make sure this is a line with data\n",
    "                        if line[9] != \"Null\" or \"us_state\":\n",
    "                            if is_topic(line[1], 5) > 0.05:\n",
    "                                #calls text of each tweet as a TextBlob object\n",
    "                                avgSentiment = (text.sentiment.polarity)\n",
    "                                sentimementDict[earlierWeekString][line[9]][0] = avgSentiment\n",
    "                            elif is_topic(line[1], 11) > 0.05:\n",
    "                                #calls text of each tweet as a TextBlob object\n",
    "                                text = textblob.TextBlob(line[1])\n",
    "                                #line[9] = state; if this state is already in the dictionary, the sentiment gets averaged\n",
    "                                avgSentiment = (text.sentiment.polarity)\n",
    "                                sentimementDict[earlierWeekString][line[9]][1] = avgSentiment\n",
    "                            else:\n",
    "                                #calls text of each tweet as a TextBlob object\n",
    "                                text = textblob.TextBlob(line[1])\n",
    "                                #line[9] = state; if this state is already in the dictionary, the sentiment gets averaged\n",
    "                                avgSentiment = (text.sentiment.polarity)\n",
    "                                sentimementDict[ealierWeekString][line[9]][2] = avgSentiment\n",
    "                    else:\n",
    "                        earlierWeekString = dt.strftime(previousWeekStart)\n",
    "                        if line[9] != \"Null\" or \"us_state\":\n",
    "                            if is_topic(line[1], 5) > 0.05:\n",
    "                                #calls text of each tweet as a TextBlob object\n",
    "                                if sentimementDict[earlierWeekString][line[9]][0] != 0:\n",
    "                                    avgSentiment = (sentimementDict[earlierWeekString][line[9]][0] +  (text.sentiment.polarity))/2\n",
    "                                else:\n",
    "                                    avgSentiment = (text.sentiment.polarity)\n",
    "                                sentimementDict[earlierWeekString][line[9]][0] = avgSentiment\n",
    "                            elif is_topic(line[1], 11) > 0.05:\n",
    "                                #calls text of each tweet as a TextBlob object\n",
    "                                text = textblob.TextBlob(line[1])\n",
    "                                #line[9] = state; if this state is already in the dictionary, the sentiment gets averaged\n",
    "                                if sentimementDict[earlierWeekString][line[9]][1] != 0:\n",
    "                                    avgSentiment = (sentimementDict[earlierWeekString][line[9]][1] +  (text.sentiment.polarity))/2\n",
    "                                else:\n",
    "                                    avgSentiment = (text.sentiment.polarity)\n",
    "                                sentimementDict[earlierWeekString][line[9]][1] = avgSentiment\n",
    "                            else:\n",
    "                                #calls text of each tweet as a TextBlob object\n",
    "                                text = textblob.TextBlob(line[1])\n",
    "                                #line[9] = state; if this state is already in the dictionary, the sentiment gets averaged\n",
    "                                if sentimementDict[earlierWeekString][line[9]][2] != 0:\n",
    "                                    avgSentiment = (sentimementDict[earlierWeekString][line[9]][2] +  (text.sentiment.polarity))/2\n",
    "                                else:\n",
    "                                    avgSentiment = (text.sentiment.polarity)\n",
    "                                sentimementDict[earlierWeekString][line[9]][2] = avgSentiment\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(Optional) writes dictionary to CSV file with rows of state, sentiment\n",
    "\"\"\"\n",
    "\n",
    "print(\"writing sentiments to file...\")\n",
    "with open(\"twitter_sentiments_byTopic.csv\", \"w\") as outFile:\n",
    "    writer = csv.writer(outFile)\n",
    "    writer.writerow([\"Start Date\", \"State\", \"Social Dist Sentiment\", \"Reopening Sentiment\", \"Other Sentiments\"]\n",
    "    for key, value in sentimentDict.items():\n",
    "        writer.writerow([key, value])\n",
    "print(\"finsished writing to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
