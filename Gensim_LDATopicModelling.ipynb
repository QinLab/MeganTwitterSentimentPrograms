{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Breaks the large raw tweet data file into manageable portions\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#setting file size to 10000 tweets\n",
    "chunk_size = 10000\n",
    "batch_no = 1\n",
    "\n",
    "print(\"Beginning to parse tweets...\")\n",
    "\n",
    "#Adjust path name per file, this is the small test file I am running currently\n",
    "for chunk in pd.read_csv(\"./tweets_01-04.csv\", chunksize = chunk_size, error_bad_lines=False):\n",
    "        chunk.to_csv(\"covid_data\" + str(batch_no) + \".csv\", index = False)\n",
    "        batch_no += 1\n",
    "\n",
    "print(\"Finished parsing tweets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\past\\builtins\\misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import csv\n",
    "import glob\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import spacy\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning to create corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01bc6895b764b65a615bcd5400a9f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5635.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "493525\n",
      "New coronavirus\n",
      "Transmission is spreading throughout in Japan\n",
      "There is no escape in Japan \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    " creates corpus for topic modelling \n",
    "'''\n",
    "import re\n",
    "\n",
    "#Correct Dates start at file 700\n",
    "\n",
    "sentimentDict = {}\n",
    "documents = []\n",
    "path = \"./covid_data*\"\n",
    "\n",
    "\n",
    "print(\"Beginning to create corpus...\")\n",
    "#I put the smaller chunked tweet files in to a directory called revisedCovidData\n",
    "for filename in tqdm(glob.glob(path)):\n",
    "    if int(filename[12:-4]) >= 700:\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as rawTweets:\n",
    "            #open as CSV iterator\n",
    "            readCSV = csv.reader(rawTweets)\n",
    "            next(readCSV)\n",
    "            #Iterate through individual tweets\n",
    "            tweet_count = 0\n",
    "            for line in readCSV:\n",
    "                if tweet_count%100 == 0:\n",
    "                    result = re.sub(r\"http\\S+\", \"\", line[1])\n",
    "                    documents.append(result)\n",
    "                tweet_count += 1\n",
    "    #break    \n",
    "        \n",
    "print(len(documents))\n",
    "print(documents[-21])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and token not in [\"rtrt\", \"https\", 'rt']:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['New', 'coronavirus\\nTransmission', 'is', 'spreading', 'throughout', 'in', 'Japan\\nThere', 'is', 'no', 'escape', 'in', 'Japan', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['new', 'coronavirus', 'transmiss', 'spread', 'japan', 'escap', 'japan']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[-21]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing the documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf9ef1812654197b8d9f24815eb365e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493525.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed_docs = []\n",
    "print(\"processing the documents\")\n",
    "for tweet in tqdm(documents):\n",
    "    preprocessed_docs.append(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Creating dictionary\")\n",
    "dictionary = gensim.corpora.Dictionary(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    }
   ],
   "source": [
    "#filters out any token in 15 or fewer, more than half, and only the 15,000 most common\n",
    "#This part can be looked at for changes\n",
    "dictionary.filter_extremes(no_below = 15, no_above = 0.5, keep_n = 15000)\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seoul', 'concert', 'cancel', 'coronavirus', 'instead', 'upset', 'fan', 'start', 'virtuous', 'cycl', 'donat', 'refund', 'prevent', 'spread', 'coronavirus', 'pm', 'donat', 'armi', 'amp', 'bts', 'member', 'name']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_docs[5])\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 2), (12, 1), (13, 1), (14, 1)]]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "id2word = corpora.Dictionary(preprocessed_docs)\n",
    "texts = preprocessed_docs\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import os\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "os.environ.update({'MALLET_HOME':r'C:/Users/user/Documents/TopicModelling/Covid_Data/mallet-2.0.8'}) \n",
    "#You should update this path as per the path of Mallet directory on your system.\n",
    "mallet_path = r'C:/Users/user/Documents/TopicModelling/Covid_Data/mallet-2.0.8/bin/mallet' \n",
    "#You should update this path as per the path of Mallet directory on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1,\n",
      "  [('coronavirus', 0.08983900668013145),\n",
      "   ('outbreak', 0.038771200118047425),\n",
      "   ('fight', 0.03324387552145297),\n",
      "   ('donat', 0.02935199958191538),\n",
      "   ('million', 0.028927766659596118),\n",
      "   ('win', 0.028890876840264007),\n",
      "   ('support', 0.02747369294758879),\n",
      "   ('cancel', 0.022057037808990664),\n",
      "   ('hope', 0.019520862729908113),\n",
      "   ('affect', 0.014866597190840257)]),\n",
      " (9,\n",
      "  [('amp', 0.146507251256644),\n",
      "   ('india', 0.02167560547391712),\n",
      "   ('govt', 0.014093653046008802),\n",
      "   ('lockdown', 0.01389824190095962),\n",
      "   ('pm', 0.008568027129079583),\n",
      "   ('ccp', 0.008090021405036197),\n",
      "   ('indian', 0.007648091584694197),\n",
      "   ('stand', 0.007575939777291421),\n",
      "   ('power', 0.007088915077322687),\n",
      "   ('muslim', 0.006998725318069218)]),\n",
      " (13,\n",
      "  [('health', 0.09099601029297198),\n",
      "   ('public', 0.044122854647182415),\n",
      "   ('care', 0.025664557709105505),\n",
      "   ('emerg', 0.024401543945796644),\n",
      "   ('worker', 0.02164239004697939),\n",
      "   ('risk', 0.021429920441936777),\n",
      "   ('protect', 0.01920784248919946),\n",
      "   ('diseas', 0.018850775514058407),\n",
      "   ('nation', 0.017888760357893246),\n",
      "   ('covid', 0.017634977218536792)]),\n",
      " (17,\n",
      "  [('coronavirus', 0.05633434729891118),\n",
      "   ('govern', 0.04314874297643973),\n",
      "   ('state', 0.036645587122535864),\n",
      "   ('respons', 0.035515131992791826),\n",
      "   ('pandem', 0.033375776048329996),\n",
      "   ('crisi', 0.02611594458338778),\n",
      "   ('penc', 0.01591449869481054),\n",
      "   ('hous', 0.015021074479367675),\n",
      "   ('press', 0.014100300543043903),\n",
      "   ('unit', 0.013641433003819845)]),\n",
      " (3,\n",
      "  [('covid', 0.04884422660269169),\n",
      "   ('social', 0.03182441117394488),\n",
      "   ('work', 0.029891791532330358),\n",
      "   ('distanc', 0.022489493659731317),\n",
      "   ('famili', 0.02083339663979434),\n",
      "   ('live', 0.01944470611114994),\n",
      "   ('today', 0.01873060923099363),\n",
      "   ('school', 0.017451312266968916),\n",
      "   ('friend', 0.016816221850149048),\n",
      "   ('free', 0.016020079796527968)]),\n",
      " (6,\n",
      "  [('coronavirus', 0.12603524310817246),\n",
      "   ('spread', 0.0400242458729442),\n",
      "   ('travel', 0.03408029589861267),\n",
      "   ('close', 0.03168354187670479),\n",
      "   ('infect', 0.0294414171465329),\n",
      "   ('countri', 0.021116176079467086),\n",
      "   ('border', 0.01884621807678272),\n",
      "   ('order', 0.01850912622079826),\n",
      "   ('break', 0.01773907234795302),\n",
      "   ('ban', 0.016427815954031803)]),\n",
      " (0,\n",
      "  [('trump', 0.12373497719809522),\n",
      "   ('presid', 0.04691677374743496),\n",
      "   ('realdonaldtrump', 0.03958769005086686),\n",
      "   ('american', 0.033638758478977175),\n",
      "   ('call', 0.03030175780429835),\n",
      "   ('cdc', 0.028722841377688567),\n",
      "   ('democrat', 0.02715792243713029),\n",
      "   ('hoax', 0.024907126680048262),\n",
      "   ('administr', 0.019361322706441923),\n",
      "   ('lie', 0.01926893929850199)]),\n",
      " (2,\n",
      "  [('coronavirus', 0.12055413911860702),\n",
      "   ('chines', 0.02812929912767497),\n",
      "   ('vaccin', 0.02411393328840761),\n",
      "   ('wuhan', 0.02057991447550254),\n",
      "   ('caus', 0.017135954138497514),\n",
      "   ('video', 0.014651582389530857),\n",
      "   ('start', 0.013896954470782236),\n",
      "   ('question', 0.013707521124923528),\n",
      "   ('outbreak', 0.012931154953371448),\n",
      "   ('gonna', 0.012844201942157615)]),\n",
      " (15,\n",
      "  [('home', 0.06224493222103178),\n",
      "   ('stay', 0.05064879169986172),\n",
      "   ('coronavirus', 0.04989484415873801),\n",
      "   ('hand', 0.04672032819611183),\n",
      "   ('mask', 0.03950746163872177),\n",
      "   ('sick', 0.02778617193056357),\n",
      "   ('wash', 0.02541444221617843),\n",
      "   ('face', 0.024968789013732832),\n",
      "   ('safe', 0.021199051308114246),\n",
      "   ('follow', 0.019724732837009974)]),\n",
      " (11,\n",
      "  [('virus', 0.1565839623346536),\n",
      "   ('corona', 0.09998007124529806),\n",
      "   ('human', 0.0194087387589368),\n",
      "   ('spread', 0.015849587723887105),\n",
      "   ('end', 0.014881174800089679),\n",
      "   ('god', 0.014709912064369878),\n",
      "   ('kill', 0.013053334329771068),\n",
      "   ('outbreak', 0.012449243952868495),\n",
      "   ('actual', 0.01243990234910196),\n",
      "   ('make', 0.01106357272749919)])]\n"
     ]
    }
   ],
   "source": [
    "ldamallet = gensim.models.wrappers.LdaMallet(\n",
    "   mallet_path, corpus=corpus, num_topics=20, id2word=id2word\n",
    ")\n",
    "pprint(ldamallet.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamallet.save(\"ldamallet2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 = reopen, 15 = stay home for ldamallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_topic(text, topic_num):\n",
    "    tokens = preprocess(text.lower())\n",
    "    return ldamallet[dictionary.doc2bow(token for token in tokens)][topic_num][1]\n",
    "def update(date):\n",
    "    u = date   \n",
    "    d = dt.timedelta(days=7)\n",
    "    t = u + d\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e26c455154f40b691b86e3bbcfc4c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5635.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "date value out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0c72f086769d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m                     \u001b[1;32melif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mweekEnd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                         \u001b[1;31m#add a week\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                         \u001b[0mweekStart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweekStart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m                         \u001b[1;31m#create a dictionary with each state as key and a list of 0's as values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                         \u001b[0mstatesDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-76780d23e027>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(date)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: date value out of range"
     ]
    }
   ],
   "source": [
    "#TODO: Fix sentiment averages, how to calculate averages during streaming\n",
    "#keep counter\n",
    "\n",
    "import datetime as dt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import textblob\n",
    "import glob\n",
    "\n",
    "\n",
    "path = \"./covid_data*\"\n",
    "abbr_list = ['ak', 'al', 'ar', 'az', 'ca', 'co', \n",
    "             'ct', 'dc', 'de', 'fl', 'ga', 'hi', \n",
    "             'ia', 'id', 'il', 'in', 'ks', 'ky', \n",
    "             'la', 'ma', 'md', 'me', 'mi', 'mn', \n",
    "             'ms', 'mo', 'mt', 'nc', 'ne', 'nh', \n",
    "             'nj', 'nm', 'nv', 'ny', 'nd', 'oh', \n",
    "             'ok', 'or', 'pa', 'ri', 'sc', 'sd', \n",
    "             'tn', 'tx', 'ut', 'vt', 'va', 'wa', \n",
    "             'wv', 'wi', 'wy']\n",
    "\n",
    "column_names = [\"Start Date\", \"State\", \"Social Dist Sentiment\", \"Reopening Sentiment\", \"Other Sentiments\"]\n",
    "\n",
    "#starting one week before start date so it updates correctly\n",
    "first = dt.datetime(2020, 1, 20)\n",
    "weekStart = first\n",
    "weekEnd = update(weekStart)\n",
    "weekString = ''\n",
    "startDate = dt.datetime(2020, 1, 27)\n",
    "sentimentDict = {}\n",
    "\n",
    "for filename in tqdm(glob.glob(path)):\n",
    "    #start at document 700, as any earlier is outside our range, thus futile\n",
    "    if int(filename[12:-4]) >= 700:\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as rawTweets:\n",
    "            #open as CSV iterator\n",
    "            readCSV = csv.reader(rawTweets)\n",
    "            for line in readCSV:\n",
    "                #change date into datetime object\n",
    "                #Format == Thu Jan 23 15:41:43 +0000 2020 \n",
    "                if line[4] != 'created_at':\n",
    "                    date = dt.datetime.strptime(line[4], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "                    #make sure they are the same week\n",
    "                    if date >= weekStart and date < weekEnd:\n",
    "                        weekString = dt.datetime.strftime(weekStart, \"%Y, %m, %d\")\n",
    "                        if line[8] in abbr_list:\n",
    "                            if is_topic(line[1], 15) > 0.05: #Check this value maybe too high\n",
    "                                #calls text of each tweet as a TextBlob object\n",
    "                                text = textblob.TextBlob(line[1])\n",
    "                                #week - state - 3 avg sentiments\n",
    "                                if sentimementDict[weekString][line[8]][0][0] != 0:\n",
    "                                    sentimementDict[weekString][line[8]][0].append(text.sentiment.polarity)\n",
    "                                else:\n",
    "                                    sentimementDict[weekString][line[8]][0][0] = (text.sentiment.polarity)\n",
    "                            elif is_topic(line[1], 0) > 0.05:\n",
    "                                #calls text of each tweet as a TextBlob object\n",
    "                                text = textblob.TextBlob(line[1])\n",
    "                                #line[8] = state; if this state is already in the dictionary, the sentiment gets averaged\n",
    "                                if sentimementDict[weekString][line[8]][1][0] != 0:\n",
    "                                    sentimementDict[weekString][line[8]][1].append((text.sentiment.polarity))\n",
    "                                else:\n",
    "                                    sentimementDict[weekString][line[8]][1][0]  = (text.sentiment.polarity)\n",
    "                            else:\n",
    "                                #calls text of each tweet as a TextBlob object\n",
    "                                text = textblob.TextBlob(line[1])\n",
    "                                #line[8] = state; if this state is already in the dictionary, the sentiment gets averaged\n",
    "                                if sentimementDict[weekString][line[8]][2][0] != 0:\n",
    "                                    sentimementDict[weekString][line[8]][2].append(text.sentiment.polarity)\n",
    "                                else:\n",
    "                                    sentimementDict[weekString][line[8]][2][0] = (text.sentiment.polarity)\n",
    "                    #if tweet is from the next week, updates the start date\n",
    "                    elif date >= weekEnd:\n",
    "                        #add a week\n",
    "                        weekStart = update(weekStart)\n",
    "                        #create a dictionary with each state as key and a list of 0's as values\n",
    "                        statesDict = {}\n",
    "                        #iterate each state name into the keys\n",
    "                        for state in abbr_list:\n",
    "                            #initialize the dictionary\n",
    "                            statesDict[state] = [[0], [0], [0]]\n",
    "                        #add this dictionary to the larger one\n",
    "                        weekString = dt.datetime.strftime(weekStart, \"%Y, %m, %d\")\n",
    "                        sentimentDict.update({weekString: statesDict})\n",
    "                        #make sure this is a line with data\n",
    "                        if line[8] in abbr_list:\n",
    "                            if is_topic(line[1], 15) > 0.05:\n",
    "                                #calls text of each tweet as a TextBlob object\n",
    "                                text = textblob.TextBlob(line[1])\n",
    "                                avgSentiment = (text.sentiment.polarity)\n",
    "                                sentimentDict[weekString][line[8]][0][0] = avgSentiment\n",
    "                            elif is_topic(line[1], 0) > 0.05:\n",
    "                                #calls text of each tweet as a TextBlob object\n",
    "                                text = textblob.TextBlob(line[1])\n",
    "                                #line[8] = state; if this state is already in the dictionary, the sentiment gets averaged\n",
    "                                avgSentiment = (text.sentiment.polarity)\n",
    "                                sentimentDict[weekString][line[8]][1][0] = avgSentiment\n",
    "                            else:\n",
    "                                #calls text of each tweet as a TextBlob object\n",
    "                                text = textblob.TextBlob(line[1])\n",
    "                                #line[8] = state; if this state is already in the dictionary, the sentiment gets averaged\n",
    "                                avgSentiment = (text.sentiment.polarity)\n",
    "                                sentimentDict[weekString][line[8]][2][0] = avgSentiment\n",
    "                    elif date <= weekStart and date >= startDate:\n",
    "                        oneWeek = dt.timedelta(days = 7)\n",
    "                        previousWeekEnd = weekEnd - oneWeek\n",
    "                        while (date >= previousWeekEnd):\n",
    "                            previousWeekEnd = previousWeekEnd - oneWeek\n",
    "                            previousWeekStart = previousWeekEnd - oneWeek\n",
    "                        earlierWeekString = dt.strftime(previousWeekStart, \"%Y, %m, %d\")\n",
    "                        if earlierWeekString not in SenimentDict:\n",
    "                            sentimentDict.update({earlierWeekString: statesDict})\n",
    "                            #make sure this is a line with data\n",
    "                            if line[8] in abbr_list:\n",
    "                                if is_topic(line[1], 15) > 0.05:\n",
    "                                    #calls text of each tweet as a TextBlob object\n",
    "                                    text = textblob.TextBlob(line[1])\n",
    "                                    avgSentiment = (text.sentiment.polarity)\n",
    "                                    sentimentDict[earlierWeekString][line[8]][0][0] = avgSentiment\n",
    "                                elif is_topic(line[1], 0) > 0.05:\n",
    "                                    #calls text of each tweet as a TextBlob object\n",
    "                                    text = textblob.TextBlob(line[1])\n",
    "                                    #line[8] = state; if this state is already in the dictionary, the sentiment gets averaged\n",
    "                                    avgSentiment = (text.sentiment.polarity)\n",
    "                                    sentimentDict[earlierWeekString][line[8]][1][0] = avgSentiment\n",
    "                                else:\n",
    "                                    #calls text of each tweet as a TextBlob object\n",
    "                                    text = textblob.TextBlob(line[1])\n",
    "                                    #line[8] = state; if this state is already in the dictionary, the sentiment gets averaged\n",
    "                                    avgSentiment = (text.sentiment.polarity)\n",
    "                                    sentimentDict[ealierWeekString][line[8]][2][0] = avgSentiment\n",
    "                        else:\n",
    "                            earlierWeekString = dt.strftime(previousWeekStart, \"%Y, %m, %d\")\n",
    "                            if line[8] in abbr_list:\n",
    "                                if is_topic(line[1], 15) > 0.05:\n",
    "                                    text = textblob.TextBlob(line[1])\n",
    "                                    #calls text of each tweet as a TextBlob object\n",
    "                                    if sentimentDict[earlierWeekString][line[8]][0][0] != 0:\n",
    "                                        sentimementDict[earlierWeekString][line[8]][0].append(text.sentiment.polarity)\n",
    "                                    else:\n",
    "                                        sentimentDict[earlierWeekString][line[8]][0][0] = (text.sentiment.polarity)\n",
    "                                elif is_topic(line[1], 0) > 0.05:\n",
    "                                    #calls text of each tweet as a TextBlob object\n",
    "                                    text = textblob.TextBlob(line[1])\n",
    "                                    #line[8] = state; if this state is already in the dictionary, the sentiment gets averaged\n",
    "                                    if sentimentDict[earlierWeekString][line[8]][1][0] != 0:\n",
    "                                        sentimentDict[earlierWeekString][line[8]][1].append(text.sentiment.polarity)\n",
    "                                    else:\n",
    "                                        sentimentDict[earlierWeekString][line[8]][1][0] = (text.sentiment.polarity)\n",
    "                                else:\n",
    "                                    #calls text of each tweet as a TextBlob object\n",
    "                                    text = textblob.TextBlob(line[1])\n",
    "                                    #line[8] = state; if this state is already in the dictionary, the sentiment gets averaged\n",
    "                                    if sentimentDict[earlierWeekString][line[8]][2][0] != 0:\n",
    "                                        sentimentDict[earlierWeekString][line[8]][2].append(text.sentiment.polarity)\n",
    "                                    else:\n",
    "                                        sentimentDict[earlierWeekString][line[8]][2][0] = (text.sentiment.polarity)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(Optional) writes dictionary to CSV file with rows of state, sentiment\n",
    "\"\"\"\n",
    "\n",
    "print(\"writing sentiments to file...\")\n",
    "with open(\"twitter_sentiments_byTopic.csv\", \"w\") as outFile:\n",
    "    writer = csv.writer(outFile)\n",
    "    writer.writerow([\"Start Date\", \"State\", \"Social Dist Sentiment\", \"Reopening Sentiment\", \"Other Sentiments\"]\n",
    "    for key, value in sentimentDict.items():\n",
    "        for state, values in value:\n",
    "                    for a, b, c in values:\n",
    "                        writer.writerow([key, state, mean(a), mean(b), mean(c)])\n",
    "print(\"finsished writing to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
